{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPool1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from events_to_pd import events_to_pd\n",
    "from gen_feats import gen_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num Background train jets = 30215\n",
      "num Signal train jets = 23727\n",
      "num Background test jets = 29767\n",
      "num Signal test jets = 23671\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bkg_path = \"bb1/run_a0.txt\"\n",
    "sig_path = \"dark1/run_a0.txt\"\n",
    "bkg_path_test = \"bb2/run_a0.txt\"\n",
    "sig_path_test = \"dark2/run_a0.txt\"\n",
    "\n",
    "max_ev = int(1e5)\n",
    "\n",
    "bkg, bkg_j = events_to_pd(bkg_path,max_ev)\n",
    "sig, sig_j = events_to_pd(sig_path,max_ev)\n",
    "bkg_test, bkg_j_test = events_to_pd(bkg_path_test,max_ev)\n",
    "sig_test, sig_j_test = events_to_pd(sig_path_test,max_ev)\n",
    "\n",
    "print(\"num Background train jets = {}\".format(len(bkg_j)))\n",
    "print(\"num Signal train jets = {}\".format(len(sig_j)))\n",
    "print(\"num Background test jets = {}\".format(len(bkg_j_test)))\n",
    "print(\"num Signal test jets = {}\".format(len(sig_j_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg.to_pickle(\"data/bkg\")\n",
    "bkg_j.to_pickle(\"data/bkg_j\")\n",
    "sig.to_pickle(\"data/sig\")\n",
    "sig_j.to_pickle(\"data/sig_j\")\n",
    "bkg_test.to_pickle(\"data/bkg_test\")\n",
    "bkg_j_test.to_pickle(\"data/bkg_j_test\")\n",
    "sig_test.to_pickle(\"data/sig_test\")\n",
    "sig_j_test.to_pickle(\"data/sig_j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bkg = pd.read_pickle(\"data/bkg\")\n",
    "bkg_j = pd.read_pickle(\"data/bkg_j\")\n",
    "sig = pd.read_pickle(\"data/sig\")\n",
    "sig_j = pd.read_pickle(\"data/sig_j\")\n",
    "bkg_test = pd.read_pickle(\"data/bkg_test\")\n",
    "bkg_j_test = pd.read_pickle(\"data/bkg_j_test\")\n",
    "sig_test = pd.read_pickle(\"data/sig_test\")\n",
    "sig_j_test = pd.read_pickle(\"data/sig_j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_constits = 25\n",
    "\n",
    "bkg, bkg_j = gen_feats(bkg, bkg_j, n_constits=n_constits, label=0, boost_and_shift=True, sort=\"absD0\")\n",
    "sig, sig_j = gen_feats(sig, sig_j, n_constits=n_constits, label=1, boost_and_shift=True, sort=\"absD0\")\n",
    "\n",
    "bkg_test, bkg_j_test = gen_feats(bkg_test, bkg_j_test, n_constits=n_constits, label=0, boost_and_shift=True, sort=\"absD0\")\n",
    "sig_test, sig_j_test = gen_feats(sig_test, sig_j_test, n_constits=n_constits, label=1, boost_and_shift=True, sort=\"absD0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>Jet</th>\n",
       "      <th>PT</th>\n",
       "      <th>Eta</th>\n",
       "      <th>Phi</th>\n",
       "      <th>mult_track</th>\n",
       "      <th>mult_tower</th>\n",
       "      <th>sumPx</th>\n",
       "      <th>sumPy</th>\n",
       "      <th>PT_track</th>\n",
       "      <th>...</th>\n",
       "      <th>DeltaR_track</th>\n",
       "      <th>D0_track</th>\n",
       "      <th>DZ_track</th>\n",
       "      <th>PT_tower</th>\n",
       "      <th>Eta_tower</th>\n",
       "      <th>Phi_tower</th>\n",
       "      <th>DeltaR_tower</th>\n",
       "      <th>MET</th>\n",
       "      <th>maxD0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154.765</td>\n",
       "      <td>-0.949109</td>\n",
       "      <td>-2.727570</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>-138.119331</td>\n",
       "      <td>-60.693523</td>\n",
       "      <td>[57.2726, 12.9823, 9.08256, 5.47551, 4.66965, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0678794, 0.0610078, 0.0680169, 0.0543531, 0...</td>\n",
       "      <td>[0.0926393, 0.00104306, 0.00190373, -0.0426708...</td>\n",
       "      <td>[-0.18709, 0.200578, -0.136752, -0.129054, -0....</td>\n",
       "      <td>[92.7263, 10.3008, 8.77416, 7.62353, 5.96511, ...</td>\n",
       "      <td>[-0.948645, -0.966066, -0.892382, -0.935221, -...</td>\n",
       "      <td>[-2.73277, -2.75509, -2.78226, -2.69846, -2.67...</td>\n",
       "      <td>[0.00522717, 0.0323247, 0.0787963, 0.0322513, ...</td>\n",
       "      <td>150.866342</td>\n",
       "      <td>0.092639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>117.169</td>\n",
       "      <td>-1.728220</td>\n",
       "      <td>0.614835</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>92.876916</td>\n",
       "      <td>65.584130</td>\n",
       "      <td>[31.0997, 23.8363, 7.36628, 5.6948, 3.8901, 3....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0417874, 0.0702219, 0.0637029, 0.117505, 0....</td>\n",
       "      <td>[0.0345435, 0.0702087, -0.0861242, -0.0495073,...</td>\n",
       "      <td>[-0.114731, -0.508576, -0.539706, 0.0398727, 0...</td>\n",
       "      <td>[65.9337, 14.1232, 12.1697, 11.946, 3.96994, 1...</td>\n",
       "      <td>[-1.69938, -1.73149, -1.74456, -1.73312, -1.67...</td>\n",
       "      <td>[0.592044, 0.604753, 0.798072, 0.557527, 0.647...</td>\n",
       "      <td>[0.036757, 0.0105981, 0.183964, 0.0575174, 0.0...</td>\n",
       "      <td>113.698723</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>192.534</td>\n",
       "      <td>0.881347</td>\n",
       "      <td>-2.883480</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>-182.341271</td>\n",
       "      <td>-48.137319</td>\n",
       "      <td>[31.9853, 15.173, 12.7997, 12.1929, 11.9221, 1...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0506226, 0.0148335, 0.0638486, 0.0553604, 0...</td>\n",
       "      <td>[-0.101334, 0.0758976, -0.0906685, 1.2702, -0....</td>\n",
       "      <td>[0.833184, -0.420616, 0.447464, -1.25446, 0.02...</td>\n",
       "      <td>[78.1232, 31.9573, 17.7868, 16.2995, 6.45135, ...</td>\n",
       "      <td>[0.823999, 0.958713, 0.845423, 0.813869, 0.901...</td>\n",
       "      <td>[-2.87367, -2.86085, -2.84399, -2.71754, -2.79...</td>\n",
       "      <td>[0.0581816, 0.0806095, 0.0533902, 0.179135, 0....</td>\n",
       "      <td>188.588283</td>\n",
       "      <td>-0.101334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>174.052</td>\n",
       "      <td>0.751863</td>\n",
       "      <td>0.214220</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>166.152380</td>\n",
       "      <td>36.147836</td>\n",
       "      <td>[22.6572, 19.2685, 18.5948, 16.2527, 8.93074, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0465386, 0.0325494, 0.046352, 0.0651489, 0....</td>\n",
       "      <td>[-0.0134758, -0.0203993, 0.145087, -0.00320931...</td>\n",
       "      <td>[0.058743, -0.0605345, -0.019895, -0.0110237, ...</td>\n",
       "      <td>[79.731, 29.4728, 16.4945, 10.0371, 8.79076, 5...</td>\n",
       "      <td>[0.792556, 0.710023, 0.730408, 0.649827, 0.744...</td>\n",
       "      <td>[0.269854, 0.192804, 0.000681421, 0.120946, 0....</td>\n",
       "      <td>[0.068928, 0.0470026, 0.214614, 0.138244, 0.01...</td>\n",
       "      <td>170.039052</td>\n",
       "      <td>-0.013476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>162.934</td>\n",
       "      <td>-1.164060</td>\n",
       "      <td>-2.494510</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-126.977381</td>\n",
       "      <td>-95.945945</td>\n",
       "      <td>[30.0525, 7.27038, 4.28302, 3.95571, 3.3523, 2...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0317486, 0.0264253, 0.0725693, 0.118105, 0....</td>\n",
       "      <td>[0.106265, -0.0453424, 0.00193799, -0.0399016,...</td>\n",
       "      <td>[-0.2378, -0.234499, -0.0835195, -0.00895009, ...</td>\n",
       "      <td>[55.6416, 28.0006, 17.5064, 14.307, 13.5087, 6...</td>\n",
       "      <td>[-1.10117, -1.19329, -1.18089, -1.2124, -1.200...</td>\n",
       "      <td>[-2.482, -2.54561, -2.51794, -2.46633, -2.5089...</td>\n",
       "      <td>[0.0641177, 0.0588712, 0.0288496, 0.055952, 0....</td>\n",
       "      <td>159.150493</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Event  Jet       PT       Eta       Phi  mult_track  mult_tower  \\\n",
       "0      1    1  154.765 -0.949109 -2.727570           9          15   \n",
       "1      1    2  117.169 -1.728220  0.614835           8          13   \n",
       "2      2    1  192.534  0.881347 -2.883480          11          25   \n",
       "3      2    2  174.052  0.751863  0.214220          12          19   \n",
       "4      3    1  162.934 -1.164060 -2.494510          10          15   \n",
       "\n",
       "        sumPx      sumPy                                           PT_track  \\\n",
       "0 -138.119331 -60.693523  [57.2726, 12.9823, 9.08256, 5.47551, 4.66965, ...   \n",
       "1   92.876916  65.584130  [31.0997, 23.8363, 7.36628, 5.6948, 3.8901, 3....   \n",
       "2 -182.341271 -48.137319  [31.9853, 15.173, 12.7997, 12.1929, 11.9221, 1...   \n",
       "3  166.152380  36.147836  [22.6572, 19.2685, 18.5948, 16.2527, 8.93074, ...   \n",
       "4 -126.977381 -95.945945  [30.0525, 7.27038, 4.28302, 3.95571, 3.3523, 2...   \n",
       "\n",
       "   ...                                       DeltaR_track  \\\n",
       "0  ...  [0.0678794, 0.0610078, 0.0680169, 0.0543531, 0...   \n",
       "1  ...  [0.0417874, 0.0702219, 0.0637029, 0.117505, 0....   \n",
       "2  ...  [0.0506226, 0.0148335, 0.0638486, 0.0553604, 0...   \n",
       "3  ...  [0.0465386, 0.0325494, 0.046352, 0.0651489, 0....   \n",
       "4  ...  [0.0317486, 0.0264253, 0.0725693, 0.118105, 0....   \n",
       "\n",
       "                                            D0_track  \\\n",
       "0  [0.0926393, 0.00104306, 0.00190373, -0.0426708...   \n",
       "1  [0.0345435, 0.0702087, -0.0861242, -0.0495073,...   \n",
       "2  [-0.101334, 0.0758976, -0.0906685, 1.2702, -0....   \n",
       "3  [-0.0134758, -0.0203993, 0.145087, -0.00320931...   \n",
       "4  [0.106265, -0.0453424, 0.00193799, -0.0399016,...   \n",
       "\n",
       "                                            DZ_track  \\\n",
       "0  [-0.18709, 0.200578, -0.136752, -0.129054, -0....   \n",
       "1  [-0.114731, -0.508576, -0.539706, 0.0398727, 0...   \n",
       "2  [0.833184, -0.420616, 0.447464, -1.25446, 0.02...   \n",
       "3  [0.058743, -0.0605345, -0.019895, -0.0110237, ...   \n",
       "4  [-0.2378, -0.234499, -0.0835195, -0.00895009, ...   \n",
       "\n",
       "                                            PT_tower  \\\n",
       "0  [92.7263, 10.3008, 8.77416, 7.62353, 5.96511, ...   \n",
       "1  [65.9337, 14.1232, 12.1697, 11.946, 3.96994, 1...   \n",
       "2  [78.1232, 31.9573, 17.7868, 16.2995, 6.45135, ...   \n",
       "3  [79.731, 29.4728, 16.4945, 10.0371, 8.79076, 5...   \n",
       "4  [55.6416, 28.0006, 17.5064, 14.307, 13.5087, 6...   \n",
       "\n",
       "                                           Eta_tower  \\\n",
       "0  [-0.948645, -0.966066, -0.892382, -0.935221, -...   \n",
       "1  [-1.69938, -1.73149, -1.74456, -1.73312, -1.67...   \n",
       "2  [0.823999, 0.958713, 0.845423, 0.813869, 0.901...   \n",
       "3  [0.792556, 0.710023, 0.730408, 0.649827, 0.744...   \n",
       "4  [-1.10117, -1.19329, -1.18089, -1.2124, -1.200...   \n",
       "\n",
       "                                           Phi_tower  \\\n",
       "0  [-2.73277, -2.75509, -2.78226, -2.69846, -2.67...   \n",
       "1  [0.592044, 0.604753, 0.798072, 0.557527, 0.647...   \n",
       "2  [-2.87367, -2.86085, -2.84399, -2.71754, -2.79...   \n",
       "3  [0.269854, 0.192804, 0.000681421, 0.120946, 0....   \n",
       "4  [-2.482, -2.54561, -2.51794, -2.46633, -2.5089...   \n",
       "\n",
       "                                        DeltaR_tower         MET     maxD0  \\\n",
       "0  [0.00522717, 0.0323247, 0.0787963, 0.0322513, ...  150.866342  0.092639   \n",
       "1  [0.036757, 0.0105981, 0.183964, 0.0575174, 0.0...  113.698723  0.034543   \n",
       "2  [0.0581816, 0.0806095, 0.0533902, 0.179135, 0....  188.588283 -0.101334   \n",
       "3  [0.068928, 0.0470026, 0.214614, 0.138244, 0.01...  170.039052 -0.013476   \n",
       "4  [0.0641177, 0.0588712, 0.0288496, 0.055952, 0....  159.150493  0.106265   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkg_j.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22145, 27)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num total examples = 44290\n",
      "num train examples = 31002\n",
      "num validate examples = 13288\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feats = [\"PT_track\", \"DeltaR_track\", \"D0_track\", \"DZ_track\"]\n",
    "\n",
    "val_frac = 0.3\n",
    "\n",
    "data = pd.concat([sig_j,bkg_j.loc[range(sig_j.shape[0])]]).sample(frac=1).reset_index()\n",
    "\n",
    "train_ind = np.arange(0,int(len(data)*(1-val_frac))) \n",
    "val_ind = np.arange(int(len(data)*(1-val_frac)),len(data))\n",
    "\n",
    "\n",
    "X_train = np.concatenate(np.array(data.copy().iloc[train_ind][feats].applymap(np.array)).flatten()).reshape(len(train_ind), n_constits, len(feats))\n",
    "y_train = data.iloc[train_ind][\"label\"]\n",
    "\n",
    "X_val =np.concatenate(np.array(data.copy().iloc[val_ind][feats].applymap(np.array)).flatten()).reshape(len(val_ind), n_constits, len(feats))\n",
    "y_val = data.loc[val_ind][\"label\"]\n",
    "\n",
    "X_test_B = np.concatenate(np.array(bkg_j_test.copy()[feats].applymap(np.array)).flatten()).reshape(len(bkg_j_test), n_constits, len(feats))\n",
    "X_test_S = np.concatenate(np.array(sig_j_test.copy()[feats].applymap(np.array)).flatten()).reshape(len(sig_j_test), n_constits, len(feats))\n",
    "\n",
    "print(\"num total examples = {}\".format(len(data)))\n",
    "print(\"num train examples = {}\".format(len(train_ind)))\n",
    "print(\"num validate examples = {}\".format(len(val_ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50)                11000     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,833\n",
      "Trainable params: 11,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.6672\n",
      "Epoch 00001: val_loss improved from inf to 0.65265, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 386ms/step - loss: 0.6672 - val_loss: 0.6526\n",
      "Epoch 2/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.6381\n",
      "Epoch 00002: val_loss improved from 0.65265 to 0.63009, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 381ms/step - loss: 0.6380 - val_loss: 0.6301\n",
      "Epoch 3/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.6117\n",
      "Epoch 00003: val_loss improved from 0.63009 to 0.59846, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 389ms/step - loss: 0.6117 - val_loss: 0.5985\n",
      "Epoch 4/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.5734\n",
      "Epoch 00004: val_loss improved from 0.59846 to 0.54934, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 0.5734 - val_loss: 0.5493\n",
      "Epoch 5/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.5030\n",
      "Epoch 00005: val_loss improved from 0.54934 to 0.49421, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 14s 424ms/step - loss: 0.5030 - val_loss: 0.4942\n",
      "Epoch 6/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4629\n",
      "Epoch 00006: val_loss improved from 0.49421 to 0.47242, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 380ms/step - loss: 0.4629 - val_loss: 0.4724\n",
      "Epoch 7/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4521\n",
      "Epoch 00007: val_loss improved from 0.47242 to 0.44300, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 407ms/step - loss: 0.4520 - val_loss: 0.4430\n",
      "Epoch 8/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4280\n",
      "Epoch 00008: val_loss improved from 0.44300 to 0.44088, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 381ms/step - loss: 0.4281 - val_loss: 0.4409\n",
      "Epoch 9/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4351\n",
      "Epoch 00009: val_loss improved from 0.44088 to 0.43415, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 386ms/step - loss: 0.4351 - val_loss: 0.4341\n",
      "Epoch 10/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4148\n",
      "Epoch 00010: val_loss improved from 0.43415 to 0.42521, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 382ms/step - loss: 0.4148 - val_loss: 0.4252\n",
      "Epoch 11/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3996\n",
      "Epoch 00011: val_loss did not improve from 0.42521\n",
      "32/32 [==============================] - 11s 332ms/step - loss: 0.3996 - val_loss: 0.4379\n",
      "Epoch 12/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4067\n",
      "Epoch 00012: val_loss improved from 0.42521 to 0.41832, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 373ms/step - loss: 0.4067 - val_loss: 0.4183\n",
      "Epoch 13/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4039\n",
      "Epoch 00013: val_loss did not improve from 0.41832\n",
      "32/32 [==============================] - 10s 322ms/step - loss: 0.4039 - val_loss: 0.4594\n",
      "Epoch 14/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4112\n",
      "Epoch 00014: val_loss did not improve from 0.41832\n",
      "32/32 [==============================] - 12s 379ms/step - loss: 0.4112 - val_loss: 0.4306\n",
      "Epoch 15/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3984\n",
      "Epoch 00015: val_loss improved from 0.41832 to 0.40554, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 15s 477ms/step - loss: 0.3984 - val_loss: 0.4055\n",
      "Epoch 16/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3802\n",
      "Epoch 00016: val_loss did not improve from 0.40554\n",
      "32/32 [==============================] - 13s 420ms/step - loss: 0.3802 - val_loss: 0.4097\n",
      "Epoch 17/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3797\n",
      "Epoch 00017: val_loss improved from 0.40554 to 0.39678, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 0.3797 - val_loss: 0.3968\n",
      "Epoch 18/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3694\n",
      "Epoch 00018: val_loss did not improve from 0.39678\n",
      "32/32 [==============================] - 11s 344ms/step - loss: 0.3693 - val_loss: 0.3975\n",
      "Epoch 19/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3694\n",
      "Epoch 00019: val_loss did not improve from 0.39678\n",
      "32/32 [==============================] - 10s 323ms/step - loss: 0.3694 - val_loss: 0.4373\n",
      "Epoch 20/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3983\n",
      "Epoch 00020: val_loss improved from 0.39678 to 0.39461, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 395ms/step - loss: 0.3984 - val_loss: 0.3946\n",
      "Epoch 21/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3965\n",
      "Epoch 00021: val_loss did not improve from 0.39461\n",
      "32/32 [==============================] - 14s 426ms/step - loss: 0.3965 - val_loss: 0.3965\n",
      "Epoch 22/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3916\n",
      "Epoch 00022: val_loss improved from 0.39461 to 0.38880, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 0.3915 - val_loss: 0.3888\n",
      "Epoch 23/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3652\n",
      "Epoch 00023: val_loss improved from 0.38880 to 0.38052, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 415ms/step - loss: 0.3651 - val_loss: 0.3805\n",
      "Epoch 24/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 00024: val_loss improved from 0.38052 to 0.37820, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 413ms/step - loss: 0.3545 - val_loss: 0.3782\n",
      "Epoch 25/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3498\n",
      "Epoch 00025: val_loss did not improve from 0.37820\n",
      "32/32 [==============================] - 12s 378ms/step - loss: 0.3498 - val_loss: 0.3830\n",
      "Epoch 26/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3408\n",
      "Epoch 00026: val_loss improved from 0.37820 to 0.36116, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 14s 424ms/step - loss: 0.3407 - val_loss: 0.3612\n",
      "Epoch 27/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3203\n",
      "Epoch 00027: val_loss improved from 0.36116 to 0.33577, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 14s 435ms/step - loss: 0.3203 - val_loss: 0.3358\n",
      "Epoch 28/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.2990\n",
      "Epoch 00028: val_loss did not improve from 0.33577\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.2990 - val_loss: 0.3682\n",
      "Epoch 29/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3154\n",
      "Epoch 00029: val_loss improved from 0.33577 to 0.31818, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 0.3154 - val_loss: 0.3182\n",
      "Epoch 30/30\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.2802\n",
      "Epoch 00030: val_loss did not improve from 0.31818\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.2802 - val_loss: 0.3268\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_constits, len(feats)), return_sequences=False))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Compile\n",
    "sgd = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('lstm_WandB', verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, verbose=1, mode=\"auto\", baseline=None, restore_best_weights=False)\n",
    "\n",
    "#Train model\n",
    "train_history = model.fit(X_train, y_train, batch_size=1000, epochs=30, validation_data=(X_val,y_val), callbacks=[checkpoint, es])\n",
    "\n",
    "#Load best weights\n",
    "model = tf.keras.models.load_model(\"lstm_WandB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3650\n",
      "Epoch 00001: val_loss did not improve from 0.38876\n",
      "32/32 [==============================] - 10s 306ms/step - loss: 0.3650 - val_loss: 0.4211\n",
      "Epoch 2/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3791\n",
      "Epoch 00002: val_loss did not improve from 0.38876\n",
      "32/32 [==============================] - 10s 300ms/step - loss: 0.3791 - val_loss: 0.3912\n",
      "Epoch 3/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3683\n",
      "Epoch 00003: val_loss improved from 0.38876 to 0.37384, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 11s 339ms/step - loss: 0.3683 - val_loss: 0.3738\n",
      "Epoch 4/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3419\n",
      "Epoch 00004: val_loss did not improve from 0.37384\n",
      "32/32 [==============================] - 10s 300ms/step - loss: 0.3419 - val_loss: 0.3962\n",
      "Epoch 5/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3567\n",
      "Epoch 00005: val_loss improved from 0.37384 to 0.37338, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 11s 349ms/step - loss: 0.3567 - val_loss: 0.3734\n",
      "Epoch 6/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3630\n",
      "Epoch 00006: val_loss improved from 0.37338 to 0.36042, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 361ms/step - loss: 0.3630 - val_loss: 0.3604\n",
      "Epoch 7/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3306\n",
      "Epoch 00007: val_loss improved from 0.36042 to 0.35092, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.3306 - val_loss: 0.3509\n",
      "Epoch 8/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3276\n",
      "Epoch 00008: val_loss did not improve from 0.35092\n",
      "32/32 [==============================] - 10s 300ms/step - loss: 0.3276 - val_loss: 0.4039\n",
      "Epoch 9/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3420\n",
      "Epoch 00009: val_loss improved from 0.35092 to 0.34307, saving model to lstm_WandB\n",
      "INFO:tensorflow:Assets written to: lstm_WandB\\assets\n",
      "32/32 [==============================] - 11s 353ms/step - loss: 0.3420 - val_loss: 0.3431\n",
      "Epoch 10/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3215\n",
      "Epoch 00010: val_loss did not improve from 0.34307\n",
      "32/32 [==============================] - 10s 304ms/step - loss: 0.3215 - val_loss: 0.6442\n",
      "Epoch 11/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.4100\n",
      "Epoch 00011: val_loss did not improve from 0.34307\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 0.4100 - val_loss: 0.4020\n",
      "Epoch 12/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3698\n",
      "Epoch 00012: val_loss did not improve from 0.34307\n",
      "32/32 [==============================] - 9s 293ms/step - loss: 0.3698 - val_loss: 0.3682\n",
      "Epoch 13/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3375\n",
      "Epoch 00013: val_loss did not improve from 0.34307\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 0.3374 - val_loss: 0.3493\n",
      "Epoch 14/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00014: val_loss did not improve from 0.34307\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 0.3080 - val_loss: 0.3914\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "train_history = model.fit(X_train, y_train, batch_size=1000, epochs=50, validation_data=(X_val,y_val), callbacks=[checkpoint, es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig_eff=0.44734225275718675\n",
      "bkg_eff=0.013216166265648428\n",
      "Wall time: 13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  859.,   601.,   519.,   486.,   763.,   880.,  1003.,  1298.,\n",
       "         2799., 12916.]),\n",
       " array([5.3676963e-04, 1.0048309e-01, 2.0042941e-01, 3.0037573e-01,\n",
       "        4.0032205e-01, 5.0026840e-01, 6.0021472e-01, 7.0016104e-01,\n",
       "        8.0010736e-01, 9.0005368e-01, 1.0000000e+00], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "model = tf.keras.models.load_model(\"lstm_WandB\")\n",
    "thresh = 0.95\n",
    "\n",
    "bkg_preds = model.predict(X_test_B).flatten()\n",
    "sig_preds = model.predict(X_test_S).flatten()\n",
    "bkg_eff = sum(bkg_preds>thresh)/len(bkg_preds)\n",
    "sig_eff = sum(sig_preds>thresh)/len(sig_preds)\n",
    "\n",
    "print(\"sig_eff={}\".format(sig_eff))\n",
    "print(\"bkg_eff={}\".format(bkg_eff))\n",
    "\n",
    "plt.hist(bkg_preds, bins=10);\n",
    "plt.hist(sig_preds, bins=10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x146c8a83e88>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjBUlEQVR4nO3deXxV9Z3/8dcnG1lIQkL2hJ2wBRA1oggVF1C0KGo32to6U1vtVFudaaeVOrU6dWF+XcZWpa3VmdrfVBn7E4Si4oJYQakYlX1LIhGykRAgG9nz/f2RWxsxwAVyc3Jv3s/HI497z7nn3rxzHsk7555z7veYcw4REQktYV4HEBGR3qdyFxEJQSp3EZEQpHIXEQlBKncRkRAU4XUAgJSUFDdy5EivY4iIBJV33333oHMutafH+kW5jxw5koKCAq9jiIgEFTP78HiPabeMiEgIUrmLiIQglbuISAhSuYuIhCCVu4hICFK5i4iEIL/K3cxKzGyrmW0yswLfvGQze8XMCn23Sd2WX2RmRWa228yuCFR4ERHp2alsuV/inJvmnMv3Td8JrHHO5QJrfNOY2SRgIZAHzAOWmFl4L2b+yOHGVu5ZuZ3aprZAvLyISNA6k90yC4AnffefBK7tNn+pc67FObcXKAKmn8H3Oa7Sw038YUMJ/7F6VyBeXkQkaPlb7g542czeNbObffPSnXMVAL7bNN/8bGB/t+eW+ub1uik5iXxt5iieensf75QcCsS3EBEJSv6W+0zn3DnAlcCtZnbRCZa1HuZ94nJPZnazmRWYWUF1dbWfMT7pn+eOI3tIDHc+u4WW9o7Tfh0RkVDiV7k758p9t1XAcrp2sxwws0wA322Vb/FSYFi3p+cA5T285mPOuXznXH5qao/j3vglblAE9107meLqRn79evFpv46ISCg5abmbWZyZxf/tPnA5sA1YCdzoW+xGYIXv/kpgoZkNMrNRQC6wsbeDd3fJhDSuPiuLJWuLKaqqD+S3EhEJCv5suacD681sM10l/bxzbjWwGJhrZoXAXN80zrntwDPADmA1cKtzLuD7S+6eP4mYqHB+uGwbnZ266LeIDGzmnPdFmJ+f73pjyN9n3tnP95/dwoPXT+GL04f3QjIRkf7LzN7tdnr6x4TUJ1Q/l5/DBaOTeeCFnVTVNXsdR0TEMyFV7mbGA9dNoaW9k3tX7fA6joiIZ0Kq3AFGpw7m25eM5fktFazZecDrOCIingi5cge4ZfYYctMG86PnttHQ0u51HBGRPheS5R4VEcbiz0yhoq6Zn7+82+s4IiJ9LiTLHeDcEcnccP4Ifv9WCZv2H/E6johInwrZcgf413njSYsfxKJlW2nr6PQ6johInwnpck+IjuTeayazs6KOJ9bv9TqOiEifCelyB5g3OYPLJ6Xz0Kt7+LCm0es4IiJ9IuTLHeDeBXlEhIVx1/Jt9IdP5IqIBNqAKPfMxBi+P28864sOsvz9Mq/jiIgE3IAod4Abzh/BOcOH8JNVOzjU2Op1HBGRgBow5R4WZjx4/VTqm9u573kNTSAioW3AlDvA+Ix4vjl7DMveK2N94UGv44iIBMyAKneA2y4dy6iUOH64fCtNrbosn4iEpgFX7tGR4dx/3WT2HTrKr14r9DqOiEhADLhyB7hwTAqfOzeHx974gB3ldV7HERHpdQOy3AF+eNVEhsREsmjZFjp0WT4RCTEDttyT4qK4++pJbC6t5Q8bSryOIyLSqwZsuQNcc1YWs8el8tOXdlN2pMnrOCIivWZAl7uZcd+1k3EO7n5OQxOISOgY0OUOMCw5ln+ZO441u6p4cVul13FERHrFgC93gH+cOZK8rAR+vHI7tU1tXscRETljKncgIjyMxddPpaahhcUv7vI6jojIGVO5+0zJSeSmWaN4euM+Nu495HUcEZEzonLv5p/njiN7SAyLlm2hpV1DE4hI8FK5dxMbFcF9102muLqRX79e7HUcEZHTpnI/xiXj07jmrCyWrC2mqKre6zgiIqdF5d6DH82fRExUOD9cto1ODU0gIkFI5d6D1PhB3HXVRDaWHOJ/C/Z7HUdE5JSp3I/jc/k5XDA6mQde2ElVXbPXcURETonK/TjMjAeum0JLeyf3/lmX5ROR4OJ3uZtZuJm9b2arfNPJZvaKmRX6bpO6LbvIzIrMbLeZXRGI4H1hdOpgvnPpWJ7fWsGrOw54HUdExG+nsuV+O7Cz2/SdwBrnXC6wxjeNmU0CFgJ5wDxgiZmF907cvnfzRWMYlz6Yu1dso6Gl3es4IiJ+8avczSwH+DTweLfZC4AnffefBK7tNn+pc67FObcXKAKm90paD0RFhPHg9VOpqGvm5y/v9jqOiIhf/N1yfwj4PtDZbV66c64CwHeb5pufDXQ/xaTUN+9jzOxmMysws4Lq6upTzd2nzh2RxA3nj+D3b5Wwaf8Rr+OIiJzUScvdzOYDVc65d/18Teth3idOFnfOPeacy3fO5aempvr50t75/rzxpMdHs2jZVto6Ok/+BBERD/mz5T4TuMbMSoClwKVm9j/AATPLBPDdVvmWLwWGdXt+DlDea4k9Eh8dyb0L8thZUcfj6/Z6HUdE5IROWu7OuUXOuRzn3Ei6DpS+5py7AVgJ3Ohb7EZghe/+SmChmQ0ys1FALrCx15N74Iq8DK7IS+ehV/fwYU2j13FERI7rTM5zXwzMNbNCYK5vGufcduAZYAewGrjVORcyQyzee81kIsPDuGu5LssnIv3XKZW7c+5159x83/0a59xlzrlc3+2hbsvd75wb45wb75x7sbdDeykjMZofzBvP+qKDLH+/zOs4IiI90idUT8OXzx/BOcOH8JNVOzjU2Op1HBGRT1C5n4awMGPxZ6bS0NLOfc9raAIR6X9U7qdpXHo835w9hmXvlbGusH+fpy8iA4/K/QzceslYRqXEcdfybTS1hswxYxEJASr3MxAdGc4D101h36Gj/HJNoddxREQ+onI/QzPGDOXz+Tn8bt0H7Civ8zqOiAigcu8VP7xqIkmxkSxatoUOXZZPRPoBlXsvGBIbxd1X57G5tJY/bCjxOo6IiMq9t1w9NZOLx6fy05d2U3akyes4IjLAqdx7iZnxkwWTcQ7ufk5DE4iIt1TuvWhYcizfvXwca3ZV8cLWSq/jiMgApnLvZf9w4UgmZyfw45XbqT3a5nUcERmgVO69LCI8jMXXT+VQYwuLV+/yOo6IDFAq9wCYnJ3ITbNG8fTGfWzce+jkTxAR6WUq9wD557njyEmKYdGyLbS0a2gCEelbKvcAiY2K4L5rJ1Nc3civXy/2Oo6IDDAq9wC6eHwaC6ZlsWRtMUVV9V7HEZEBROUeYD+aP4mYqHAWLdtKp4YmEJE+onIPsJTBg7jr0xN5p+QwS9/Z73UcERkgVO594HPn5jBj9FAefHEnVXXNXscRkQFA5d4HzIwHrp9CS3sn9/5Zl+UTkcBTufeRUSlx3H5ZLs9vreDVHQe8jiMiIU7l3oe+8anRjE+P5+4V22hoafc6joiEMJV7H4qKCOOB66dQUdfMz17a7XUcEQlhKvc+du6IJL5ywQie3FDCpv1HvI4jIiFK5e6Bf71iPOnx0dz57BbaOjq9jiMiIUjl7oH46Ej+fUEeuyrreXzdXq/jiEgIUrl75PK8DOblZfDQq3v4sKbR6zgiEmJU7h6655o8osLDuGu5LssnIr1L5e6hjMRovn/lBNYXHWT5+2VexxGREKJy99iXpw/n3BFJ/GTVDmoaWryOIyIh4qTlbmbRZrbRzDab2XYzu9c3P9nMXjGzQt9tUrfnLDKzIjPbbWZXBPIHCHZhYcaD10+hoaWd+5/f6XUcEQkR/my5twCXOufOAqYB88zsAuBOYI1zLhdY45vGzCYBC4E8YB6wxMzCA5A9ZIxLj+efZo9h2ftlrCus9jqOiISAk5a769Lgm4z0fTlgAfCkb/6TwLW++wuApc65FufcXqAImN6boUPRty4Zy+iUOO5avo2mVl2WT0TOjF/73M0s3Mw2AVXAK865t4F051wFgO82zbd4NtB94PJS37xjX/NmMysws4Lqam2tRkeG88D1U9h36Ci/XFPodRwRCXJ+lbtzrsM5Nw3IAaab2eQTLG49vUQPr/mYcy7fOZefmprqV9hQd8HooXwhfxi/W/cBO8rrvI4jIkHslM6Wcc4dAV6na1/6ATPLBPDdVvkWKwWGdXtaDlB+pkEHikVXTSApNpJFy7bQocvyichp8udsmVQzG+K7HwPMAXYBK4EbfYvdCKzw3V8JLDSzQWY2CsgFNvZy7pA1JDaKu6/OY3NpLU++VeJ1HBEJUv5suWcCa81sC/AOXfvcVwGLgblmVgjM9U3jnNsOPAPsAFYDtzrndITwFFw9NZNLxqfys5d3U3akyes4IhKErD987D0/P98VFBR4HaNfKT18lLm/eIMZY4byxI35mPV0KENEBjIze9c5l9/TY/qEaj+VkxTLdy8fx2u7qnhha6XXcUQkyKjc+7F/uHAkU7IT+fHK7dQebfM6jogEEZV7PxYRHsaD10/h8NFWFq/e5XUcEQkiKvd+bnJ2IjfNGsXTG/exce8hr+OISJBQuQeBO+bkMiw5hkXLttDSrhOPROTkVO5BIDYqgvuunUJxdSNL1hZ7HUdEgoDKPUjMHpfKtdOyWPJ6EUVV9V7HEZF+TuUeRP5t/iTiBkWwaNlWOjU0gYicgMo9iKQMHsRdV03knZLDLH1n/8mfICIDlso9yHz23BxmjB7Kgy/upKqu2es4ItJPqdyDjJnxwPVTaGnv5J4/b/c6joj0Uyr3IDQqJY7bL8vlha2VvLLjgNdxRKQfUrkHqZsvGs349HjuXrGNhpZ2r+OISD+jcg9SkeFhPPiZKVTWNfOzl3Z7HUdE+hmVexA7Z3gSX71gBE9uKGHT/iNexxGRfkTlHuS+d8V40uOjufPZLbR1dHodR0T6CZV7kIuPjuTfF+Sxq7Kex9ft9TqOiPQTKvcQcHleBldOzuChV/dQcrDR6zgi0g+o3EPEPdfkERUexl3PbaU/XDpRRLylcg8R6QnR/ODKCbxZVMOy98q8jiMiHlO5h5AvTR/OuSOSuO/5HdQ0tHgdR0Q8pHIPIWFhxoPXT6GhpZ37n9/pdRwR8ZDKPcSMS4/nn2aPYdn7ZawrrPY6joh4ROUegr51yVhGp8Zx1/JtNLXqsnwiA5HKPQRFR4bzwHVT2HfoKA+t2eN1HBHxgMo9RF0weigLzxvG4+v2sr281us4ItLHVO4hbNGVE0mKjWLRsq106LJ8IgOKyj2EJcZG8uOrJ7GltJYn3yrxOo6I9CGVe4ibPzWTS8ansvjFXSzduM/rOCLSR1TuIc7M+M8vTOP80cncuWwrP/h/W2hu0xk0IqFO5T4ADImN4vf/OJ3bLhnL/xbs53O/2cD+Q0e9jiUiAaRyHyDCw4zvXTGex7+aT0lNI1c/sp6/7NGHnERC1UnL3cyGmdlaM9tpZtvN7Hbf/GQze8XMCn23Sd2es8jMisxst5ldEcgfQE7NnEnp/Pm2WWQkRPMP/72RX60ppFNn0oiEHH+23NuB7zrnJgIXALea2STgTmCNcy4XWOObxvfYQiAPmAcsMbPwQISX0zMyJY7l35rJtdOy+cUre/j6HwqoPdrmdSwR6UUnLXfnXIVz7j3f/XpgJ5ANLACe9C32JHCt7/4CYKlzrsU5txcoAqb3cm45QzFR4fzi82fxkwV5rCus5upH1uvDTiIh5JT2uZvZSOBs4G0g3TlXAV3/AIA032LZwP5uTyv1zTv2tW42swIzK6iu1r5fL5gZX5kxkv+9ZQat7Z1cv+Qtnn231OtYItIL/C53MxsMPAvc4ZyrO9GiPcz7xE5d59xjzrl851x+amqqvzEkAM4ZnsSq78zinOFJfPdPm7lr+VZa2nW6pEgw86vczSySrmL/o3NumW/2ATPL9D2eCVT55pcCw7o9PQco7524Eigpgwfxf2+azi2zR/PHt/fx+d/+lfIjTV7HEpHT5M/ZMgY8Aex0zv2i20MrgRt9928EVnSbv9DMBpnZKCAX2Nh7kSVQIsLDWHTlRH5zwzkUVzUw/+H1vFl00OtYInIa/Nlynwl8BbjUzDb5vq4CFgNzzawQmOubxjm3HXgG2AGsBm51zuk9fhCZNzmTFbfNZGhcFF954m0eXVuk0yVFgow55/0fbX5+visoKPA6hhyjsaWdHzy7hVVbKpg7KZ2ff/4sEqIjvY4lIj5m9q5zLr+nx/QJVTmuuEERPPzFs7l7/iTW7qrimofXs6vyRMfSRaS/ULnLCZkZX5s1iqe+cQGNrR1c9+hbrNhU5nUsETkJlbv4ZfqoZJ7/9iymZCdy+9JN3LNyO63tnV7HEpHjULmL39ISovnjN87nplmj+P1bJSx8bAOVtc1exxKRHqjc5ZREhofxo/mTeORLZ7Orsp75D69jQ3GN17FE5Bgqdzkt86dmseLWmSTERHLDE2/zuzc+oD+ceSUiXVTuctpy0+NZcetM5k5M5/4XdnLrU+/R0NLudSwRQeUuZyg+OpJf33AOi66cwOptlSx4ZD1FVfVexxIZ8FTucsbMjFtmj+F/vn4+tU1tXPPIm6zaouGERLykcpdec+GYFFZ9+1NMyIjntqfe575VO2jr0OmSIl5QuUuvykiMZunNM7hxxggeX7+XL//ubarqdbqkSF9TuUuvi4oI494Fk/nPL5zFlrIjfPpX61m1pVyDj4n0IZW7BMx1Z+ew/FszGRITyW1Pvc+Vv1zHC1srVPIifUDlLgE1MTOB1XdcxC8XTqOts5Nv/fE9rvrVOlZvU8mLBJKG/JU+09HpWLm5jF+tKWLvwUYmZSZw+5xcLp+UTtc1YUTkVJxoyF+Vu/S59o5OVmwq5+HXCimpOUpeVgJ3zBnHnIlpKnmRU6Byl36pvaOT53wl/2HNUaZkJ3LHnFwunaCSF/GHyl36tbaOTpa/X8bDrxWy/1ATZ+UkcseccVw8PlUlL3ICKncJCm0dnSx7r5SHXyui9HATZw0bwh1zcrl4nEpepCcqdwkqre2dPPteKY+8VkTZkSbOHj6EO+aM46LcFJW8SDcqdwlKre2d/Ond/Tz6WhHltc2cOyKJO+bkMmusSl4EVO4S5FraO/hTQSmPri2ioraZ/BFJ3DFnHDPHDlXJy4CmcpeQ0NLewTPv7OfRtcVU1nVtyd9+WS6f0u4aGaBU7hJSmts6+FPBfpa8XkxFbTPThg3hdh14lQFI5S4hqaW9g2ffLePRtV0HXqfmJPKdS3O5TB+GkgFC5S4hrbW9k+Xvl/LI2iL2H2oiLyuB71ymYQ0k9KncZUBo6+jkufe7tuRLao4yMTOB71w6livyMggLU8lL6FG5y4DS3tHJys3lPPJaER8cbGR8ejzfvmwsV03OVMlLSFG5y4DU0elYtaWch18roqiqgdy0wdx26VjmT80iXCUvIUDlLgNaR6fjxW0V/GpNIXsONDA6JY6vzRrFZ87JISYq3Ot4IqdN5S4CdHY6Vm+v5Ld/KWZzaS1DYiP58vnD+eqMkaQnRHsdT+SUnajcT3olJjP7LzOrMrNt3eYlm9krZlbou03q9tgiMysys91mdkXv/AgiZy4szLhqSibP3TqTP31zBheMGsqS14uZ9R+v8S/PbGJ7ea3XEUV6zUm33M3sIqAB+INzbrJv3v8BDjnnFpvZnUCSc+4HZjYJeBqYDmQBrwLjnHMdJ/oe2nIXr3xY08h/v1nCMwX7OdrawYVjhvL1T43i4nFpOvgq/d4Z75Yxs5HAqm7lvhu42DlXYWaZwOvOufFmtgjAOfegb7mXgHuccxtO9Poqd/FabVMbSzfu4/dvlVBR28zo1Di+NlP75aV/O6PdMseR7pyrAPDdpvnmZwP7uy1X6pvXU6ibzazAzAqqq6tPM4ZI70iMieSW2WN44/uX8MuF0xg8KIJ/e24bMxav4Wcv7eZwY6vXEUVOyemW+/H09D62x7cGzrnHnHP5zrn81NTUXo4hcnoiw8NYMC2bFbfO5JlbZnD+qGQefb2Ii366lsfeKKa57YR7GEX6jdMt9wO+3TH4bqt880uBYd2WywHKTz+eiDfMjOmjkvntV/J56Y6LyB+RxAMv7OKyn/+FFZvK6Oz0/iwzkRM53XJfCdzou38jsKLb/IVmNsjMRgG5wMYziyjirXHp8fz3P07nj18/n8SYSG5fuonrlrzJ2x/UeB1N5Lj8ORXyaWADMN7MSs3sJmAxMNfMCoG5vmmcc9uBZ4AdwGrg1pOdKSMSLGaOTWHVt2fx88+dRVV9C1947K984w8FFFc3eB1N5BP0ISaR09Dc1sET6/fy69eLaWrr4EvTh3P7nFxSBg/yOpoMIPqEqkiAHGxo4ZevFvLUxn1Ehhufzx/G12eNZvjQWK+jyQCgchcJsOLqBn77l2KWv19GR6fjqimZ3HLRGKbkJHodTUKYyl2kjxyoa+a/3tzLU3/dR31LOxeOGcots8dwka7zKgGgchfpY/XNbTy9cR9PrN/LgboWJmTEc8vs0Xx6ShZREb398RIZqFTuIh5pbe9kxaYyHnvjAwqrGkiMieSKvHQ+PTWLC8cMJTJcRS+nT+Uu4rHOTscbhdWs3FTOyzsO0NDSTlJsJPMmZzB/ahbnj0omQkUvp0jlLtKPNLd18MaealZtqeDVnQc42tpByuAo5k3OYMG0bM4dnqQRKcUvKneRfqq5rYO1u6pYtaWCNbsO0NzWSU5SDAumZXHttGxy0+O9jij9mMpdJAg0tLTz8vZKnttUzvrCajodTMpM4Nqzs7jmrGwyEnW1KPk4lbtIkKmqb2bV5gpWbCpjc2ktZjBj9FCunZbNvCkZJERHeh1R+gGVu0gQ+6C6gRWbylmxqYySmqNERYRx2YQ0FkzL4pIJaQyK0MVEBiqVu0gIcM6xubSW594vY9WWcg42tJIQHcHs8Wl8amwKM3NTyB4S43VM6UMqd5EQ097RyZvFNazcVM4bhdVU17cAMDoljpljU5iVm8IFo4aSGKvdN6FM5S4Swpxz7DnQwPqig6wvrObtvYc42tqBGeRlJTBj9FAuHJPCeaOSGTwowuu40otU7iIDSGt7J5v2H2FDcQ1vFR/k/X1HaO3oxAxGJMcyISOB8RnxTMyMZ2JmAsOTYzXuTZBSuYsMYM1tHbz74WHeKTnE7sp6dlfWs7emkb/96acMHsR5I5M4b2Qy541MZlJWAuH6EFVQOFG56z2aSIiLjgxn5tgUZo5N+WheU2sHhVX1bC2rpaCkq/hf3FYJQGJMJJ/KTWH2uFRmj0slLUHn1wcjbbmLCAAVtU1s3HuIdYUH+cuebgdpU+OYkBHPuPR4xqfHk5sez7DkGJ2C2Q9ot4yInBLnHDsr6nl9TxWb9x9hz4EGSrrtyjGDrMQYRqbEMnJoHBMyE8jLSmBiRgIxUSr9vqLdMiJySsyMSVkJTMpK+GheU2sHxdUN7DlQz4c1R/mwppGSmqP8eXM5f3x7HwBhBiNT4hiXFs+49MHkpseTmz6YUSlx2tLvYyp3EfFLTFQ4k7MTmZz98UsHOucoO9LE9vI6tpfXsbuyjj0H6nl5RyWdvi398DBjRHIsuemDmZiZwKTMrn8c2UNidKZOgKjcReSMmBk5SbHkJMVyRV7GR/Ob2zr4oLqRouoGig7UU1jVwO7Kel7eceCj3TvxgyIYnxHP+Ix4Rgzteo3sITFkJEYzNC5KY9yfAZW7iAREdGT4J3btABxtbWdXZT07yuvYXVnPrso6/ry5nLrm9o8tF2Zdp2kOT45leHIsOcmx5AyJITsphszEaDISo4mNUoUdj9aMiPSp2KgIzhmexDnDkz42v7apjdLDRyk73ERVfQtVdc1U1Daz79BRNnxQQ+WmMo49/yMhOoLhQ2MZMTSO4cmxZCVGk5kYQ05yDMOSYokbwJ/IHbg/uYj0K4kxkSTGJJKXldjj463tnVTWNlN65CiVtc1U1jVTcaSr/LeV1fLStkraOz/e/slxUWQN6Sr89IRBpMVHkzJ4ECmDo0hLiCYnKYahcVEhud9f5S4iQSEqIozhQ2MZPjS2x8c7Ox0HG1ooO9JE6eEm9h06StmRJsqPNPFhTSMFJYc4fLTtE8+LiQz/aDdPRmI02UNiyEzs2vWT5dsNFIxj8gRfYhGRHoSFGWkJ0aQlRHP2Mbt8/qa1vZOaxhYO1rdSWddM6eGjlB5uorK2mfLaJjYU13Cgrplj3gAQHx3BkNhIhsREMSQ2ksSYSJLjohgaN4iU+CiSY6NIiosiK7HrYHBUhPcHglXuIjJgREWE+bbKY5hCz7t/2js6qapvoaK2mXLfu4DK2iZqm9o40tTmOzbQxKHGVmqbPvlOwHwHgjMTo0kdPIi0hEG+22gyEqJJT4gmLWFQwM8GUrmLiHQTER5G1pAYsobEcO6Int8B/E1reyeHj7ZyqLGVgw1d/xDKfO8EKnwHhDeX1lLT2PKJg8HhYUZa/CA+PSWTf5s/qfd/jl5/RRGRASIqIox039b4ibR3dHKwoWtX0IG6ZqrrWzhQ10z5kWYyA3T1LJW7iEiARYSHfXTAtq94v9dfRER6XcDK3czmmdluMysyszsD9X1EROSTAlLuZhYOPApcCUwCvmhmvX/EQEREehSoLffpQJFz7gPnXCuwFFgQoO8lIiLHCFS5ZwP7u02X+uZ9xMxuNrMCMyuorq4OUAwRkYEpUOXe00ANHzvL0zn3mHMu3zmXn5qaGqAYIiIDU6DKvRQY1m06BygP0PcSEZFjBKrc3wFyzWyUmUUBC4GVAfpeIiJyjIBdINvMrgIeAsKB/3LO3X+CZauBDwMSpEsKcDCArx8Iyhx4wZYXlLkvBFPeEc65HvdrB6zc+xMzKzjeFcL7K2UOvGDLC8rcF4It7/HoE6oiIiFI5S4iEoIGSrk/5nWA06DMgRdseUGZ+0Kw5e3RgNjnLiIy0AyULXcRkQFF5S4iEoJCqtxPNsywmU0wsw1m1mJm3/Mi47H8yPxlM9vi+3rLzM7yIme3PCfLu8CXdZNv7KBZXuQ8JpNfw0+b2Xlm1mFmn+3LfMfJcrL1fLGZ1frW8yYzu9uLnN3ynHQd+zJvMrPtZvaXvs7YQ56TreN/7bZ+t/l+N5K9yHpanHMh8UXXh6WKgdFAFLAZmHTMMmnAecD9wPeCJPOFQJLv/pXA2/0872D+fixnKrCrv6/jbsu9BrwAfLa/ZwYuBlZ5mfMU8w4BdgDDfdNp/T3zMctfDbzm9bo+la9Q2nI/6TDDzrkq59w7wCcvWe4NfzK/5Zw77Jv8K13j9HjFn7wNzvfXAMRxzIBxHvB3+OlvA88CVX0Z7jiCbchsf/J+CVjmnNsHXX+LfZzxWKe6jr8IPN0nyXpJKJX7SYcZ7odONfNNwIsBTXRifuU1s+vMbBfwPPC1Psp2PP4MP50NXAf8pg9znYi/vxczzGyzmb1oZnl9E61H/uQdBySZ2etm9q6ZfbXP0vXM7789M4sF5tH1zz9ohNIFsk86zHA/5HdmM7uErnL3ch+2X3mdc8uB5WZ2EfATYE6gg52AP5kfAn7gnOsw62nxPudP5vfoGlekwTeO03NAbqCDHYc/eSOAc4HLgBhgg5n91Tm3J9DhjuNU+uJq4E3n3KEA5ul1oVTuwTjMsF+ZzWwq8DhwpXOupo+y9eSU1rFz7g0zG2NmKc45rwZi8idzPrDUV+wpwFVm1u6ce65PEn7SSTM75+q63X/BzJZ4uJ79WcelwEHnXCPQaGZvAGcBXpX7qfwuLyTIdskAIXVANQL4ABjF3w+Q5B1n2XvoHwdUT5oZGA4UARcGSd6x/P2A6jlA2d+m+2vmY5b/Pd4fUPVnPWd0W8/TgX1erWc/804E1viWjQW2AZP78zr2LZcIHALivPydOJ2vkNlyd861m9ltwEv8fZjh7Wb2Td/jvzGzDKAASAA6zewOuo6Q1x3vdb3ODNwNDAWW+LYs251HI9b5mfczwFfNrA1oAr7gfH8l/Thzv+Jn5s8C/2Rm7XSt54VerWd/8jrndprZamAL0Ak87pzb5kVefzP7Fr0OeNl1veMIKhp+QEQkBIXS2TIiIuKjchcRCUEqdxGREKRyFxEJQSp3EZEQpHIXEQlBKncRkRD0/wFSpJW++0DpXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "sig_eff = []\n",
    "bkg_eff = []\n",
    "frac = []\n",
    "for thresh in (1-np.arange(0.001,0.5,0.01)):\n",
    "    bkg_eff_temp = sum(bkg_preds>thresh)/len(bkg_preds)\n",
    "    sig_eff_temp = sum(sig_preds>thresh)/len(sig_preds)\n",
    "    sig_eff.append(sig_eff_temp)\n",
    "    bkg_eff.append(1/bkg_eff_temp)\n",
    "plt.plot(sig_eff,bkg_eff)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
